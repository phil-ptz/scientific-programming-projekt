{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "In diesem Notebook wird das Klassifizierungs-Modell erstellt und Trainiert. Den Datensatz, welchen wir für das Training genutzt haben, haben wir von Kaggle:\n",
        "\n",
        "https://www.kaggle.com/datasets/xhlulu/140k-real-and-fake-faces/\n",
        "\n",
        "Der Datensatz besteht aus 265x265 Pixel großen Bildern von 70k echten Gesichtern aus der Nvidia-Datenbank \"Flickr\" und 70k KI-Generierten Bildern von \"StyleGAN\"."
      ],
      "metadata": {
        "id": "qcurXTLxQqnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "\n",
        "- Torch - Erstellung des Modells\n",
        "- Matplotlib - Darstellung der Bilder\n",
        "- Tqdm - Ladebalken beim Training"
      ],
      "metadata": {
        "id": "3EtnpcLwSHTc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhViVaXMs15q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datenset laden\n",
        "\n",
        "Den Datensatz laden wir aus Google Drive, da es mit Colab am besten funktioniert."
      ],
      "metadata": {
        "id": "Nid0EwR-M_2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp /content/drive/MyDrive/Dataset/data.zip /content\n",
        "!unzip /content/data.zip"
      ],
      "metadata": {
        "id": "I4Y-HoRtI5Xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMXpkyFxs15r"
      },
      "source": [
        "## Konstanten\n",
        "\n",
        "Diese Konstanten wurden so optimiert, damit wir die bestmögliche Accuracy erhalten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U46KFW_8s15s"
      },
      "outputs": [],
      "source": [
        "ROOT_DIR: str = \"/content/data\"\n",
        "TRAIN_TEST_RATIO: float = 0.2 # Anteil von Test split\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# Konstanten die optimiert werden sollen\n",
        "IMAGE_SIZE: int = 256\n",
        "BATCH_SIZE: int = 512 # Hohe Batch-Size => Gut für GPUs mit viel RAM\n",
        "HIDDEN_UNITS: int = 32\n",
        "LEARNING_RATE: float = 0.001\n",
        "EPOCHS: int = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Daten in ImageFolder speichern\n",
        "\n",
        "Hier werden die Daten Transformiert, um sie zu normen und um Overfitting zu verhindern."
      ],
      "metadata": {
        "id": "YdK1YZYGTdl_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2E5doa6Zs15t"
      },
      "outputs": [],
      "source": [
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), # Resize auf einheitliche Größe\n",
        "    torchvision.transforms.RandomHorizontalFlip(), # Hälfte der Bilder horizontal spiegeln\n",
        "    torchvision.transforms.RandomRotation(degrees=15), # Zufällige Rotation\n",
        "    torchvision.transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)), # Zufälliges Zuschneiden\n",
        "    torchvision.transforms.ToTensor(), # In Tensor umwandeln\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mssNftSfs15u"
      },
      "outputs": [],
      "source": [
        "# Die Klassen werden Automatisch nach den Ordernamen gespeichert (fake, real)\n",
        "dataset = torchvision.datasets.ImageFolder(root=ROOT_DIR, transform=transform)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32dQxy_7s15u"
      },
      "source": [
        "## Bilder darstellen\n",
        "\n",
        "Hier werden ein paar zufällige Bilder zur Kontrolle ausgegeben."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEOfYftEs15v"
      },
      "outputs": [],
      "source": [
        "num_images = 3\n",
        "\n",
        "for _ in range(num_images):\n",
        "\n",
        "    # Zufälliges Bild und Label speichern\n",
        "    index = random.randint(0, len(dataset) - 1)\n",
        "    img, label = dataset[index]\n",
        "\n",
        "    # Tensor in Bild umwandeln\n",
        "    img = img.permute(1, 2, 0)\n",
        "\n",
        "    # Darstelluung mit Pyplot\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Label: {dataset.classes[label]} & Index: {index}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEXINok7s15v"
      },
      "source": [
        "## Dataloader erstellen\n",
        "\n",
        "Hier werden Train- und Test-Dataloader erstellt, die einer effizienten Verarbeitung von Daten dienen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYIWzEyAs15w"
      },
      "outputs": [],
      "source": [
        "# Train/Test Split berechnen\n",
        "total_size = len(dataset)\n",
        "test_size = int(TRAIN_TEST_RATIO * total_size)\n",
        "train_size = total_size - test_size\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# DataLoader mit Batches\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LlG1SWds15w"
      },
      "source": [
        "## Ausgabe von Metadaten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4uAPSnBs15w"
      },
      "outputs": [],
      "source": [
        "print(f\"Länge Datensatz: {len(dataset)}\\n\")\n",
        "print(\"Shapes der Tensoren:\")\n",
        "print(f\"Labels: {dataset.classes}\")\n",
        "\n",
        "images, labels = next(iter(train_loader))\n",
        "\n",
        "print(f\"Bilder-Batch Shape: {images.shape}\")\n",
        "print(f\"Label-Batch Shape: {labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvG6WptGs15w"
      },
      "source": [
        "## CNN-Model Klasse erstellen\n",
        "\n",
        "Dieses Modell haben wir von https://poloclub.github.io/cnn-explainer/. Es ist aber für unsere Zwecke etwas abgeändert wurden (z.B. Dropout-Layer hinzugefügt)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46tjArqos15w"
      },
      "outputs": [],
      "source": [
        "class CNNModel(nn.Module):\n",
        "    \"\"\"\n",
        "    TinyVGG:\n",
        "    https://poloclub.github.io/cnn-explainer/\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,\n",
        "                         stride=2)\n",
        "        )\n",
        "        self.block_2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(in_features=hidden_units * (IMAGE_SIZE//4) * (IMAGE_SIZE//4),\n",
        "                    out_features=output_shape)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.block_1(x)\n",
        "        x = self.block_2(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "model = CNNModel(input_shape=3,\n",
        "    hidden_units=HIDDEN_UNITS,\n",
        "    output_shape=len(dataset.classes)).to(device)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mayZ_HX-s15x"
      },
      "source": [
        "## Loss-Function, Optimizer, Scheduler\n",
        "\n",
        "Loss-Functions und Optimizer sind bei Torch (und auch allgemein bei Neuronalen Netzen) essentiell. Wie nutzen hier die Cross-Entropy-Lossfunction und den Adam Optimizer, da diese für CNN-Klassifizierungen optimal sind.\n",
        "\n",
        "Der Learning-Rate-Scheduler ist optional, aber sehr hilfreich, denn dieser passt die Lernrate automatisch an, falls das Modell ein Plateu erreicht und sich nicht verbessert."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcaCojAQs15x"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=0.1,\n",
        "    patience=3,\n",
        "    threshold=1e-4\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqo69YCvs15x"
      },
      "source": [
        "## Hilfsfunktionen\n",
        "\n",
        "Diese Funktionen haben wir von dem Online-Kurs: https://www.learnpytorch.io/. Sie sind zum Training und Testen des Modells gedacht und dienen dazu die Trainingsschleife übersichtlich zu halten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEEpZi7ss15x"
      },
      "outputs": [],
      "source": [
        "# Evaluierung des Modells nach Training\n",
        "\n",
        "def eval_model(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               accuracy_fn):\n",
        "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
        "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
        "        loss_fn (torch.nn.Module): The loss function of model.\n",
        "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
        "\n",
        "    Returns:\n",
        "        (dict): Results of model making predictions on data_loader.\n",
        "    \"\"\"\n",
        "    loss, acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            # Make predictions with the model\n",
        "            y_pred = model(X)\n",
        "\n",
        "            # Accumulate the loss and accuracy values per batch\n",
        "            loss += loss_fn(y_pred, y)\n",
        "            acc += accuracy_fn(y_true=y,\n",
        "                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n",
        "\n",
        "        # Scale loss and acc to find the average loss/acc per batch\n",
        "        loss /= len(data_loader)\n",
        "        acc /= len(data_loader)\n",
        "\n",
        "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
        "            \"model_loss\": loss.item(),\n",
        "            \"model_acc\": acc}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jt7HYT6Ms15x"
      },
      "outputs": [],
      "source": [
        "# Training und Testen des Modells (einmal pro Epoche)\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               accuracy_fn,\n",
        "               device: torch.device = device):\n",
        "    train_loss, train_acc = 0, 0\n",
        "    model.to(device)\n",
        "    for batch, (X, y) in enumerate(data_loader):\n",
        "        # Send data to GPU\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # 2. Calculate loss\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss\n",
        "        train_acc += accuracy_fn(y_true=y,\n",
        "                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -> pred labels\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate loss and accuracy per epoch and print out what's happening\n",
        "    train_loss /= len(data_loader)\n",
        "    train_acc /= len(data_loader)\n",
        "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
        "\n",
        "def test_step(data_loader: torch.utils.data.DataLoader,\n",
        "              model: torch.nn.Module,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              accuracy_fn,\n",
        "              device: torch.device = device):\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model.to(device)\n",
        "    model.eval() # put model in eval mode\n",
        "    # Turn on inference context manager\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "            # Send data to GPU\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            test_pred = model(X)\n",
        "\n",
        "            # 2. Calculate loss and accuracy\n",
        "            test_loss += loss_fn(test_pred, y)\n",
        "            test_acc += accuracy_fn(y_true=y,\n",
        "                y_pred=test_pred.argmax(dim=1) # Go from logits -> pred labels\n",
        "            )\n",
        "\n",
        "        # Adjust metrics and print out\n",
        "        test_loss /= len(data_loader)\n",
        "        test_acc /= len(data_loader)\n",
        "        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")\n",
        "\n",
        "    return test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqT4R6wPs15y"
      },
      "outputs": [],
      "source": [
        "# Berechnung der Accuracy (Genauigkeit des Modells) zum Vergleich\n",
        "\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "    \"\"\"Calculates accuracy between truth labels and predictions.\n",
        "\n",
        "    Args:\n",
        "        y_true (torch.Tensor): Truth labels for predictions.\n",
        "        y_pred (torch.Tensor): Predictions to be compared to predictions.\n",
        "\n",
        "    Returns:\n",
        "        [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\n",
        "    \"\"\"\n",
        "    correct = torch.eq(y_true, y_pred).sum().item()\n",
        "    acc = (correct / len(y_pred)) * 100\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQkKkq_Ys15y"
      },
      "source": [
        "## Trainingsschleife\n",
        "\n",
        "Pro Epoche wird einmal Trainiert und Getestet und danach die Lernrate angepasst."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXMoUavzs15y"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "    print(f\"Epoch: {epoch}\\n---------\")\n",
        "    train_step(data_loader=train_loader,\n",
        "        model=model,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        accuracy_fn=accuracy_fn,\n",
        "        device=device\n",
        "    )\n",
        "    test_loss = test_step(data_loader=test_loader,\n",
        "        model=model,\n",
        "        loss_fn=loss_fn,\n",
        "        accuracy_fn=accuracy_fn,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    lr_scheduler.step(test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluierung und Speicherung\n",
        "\n",
        "Das Modell wird zur Sicherheit einmal komplett gespeichert (veraltet) und einmal nur die Gewichte."
      ],
      "metadata": {
        "id": "vgB7xt1CZF7J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qt50n7wIs15y"
      },
      "outputs": [],
      "source": [
        "model_results = eval_model(\n",
        "    model=model,\n",
        "    data_loader=test_loader,\n",
        "    loss_fn=loss_fn,\n",
        "    accuracy_fn=accuracy_fn\n",
        ")\n",
        "model_results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model: torch.nn.Module,\n",
        "               model_name: str):\n",
        "  torch.save(model.state_dict(), f\"/content/drive/MyDrive/Dataset/saves/model_weights_{model_name}.pth\")\n",
        "  torch.save(model, f\"/content/drive/MyDrive/Dataset/saves/full_model_weights_{model_name}.pth\")"
      ],
      "metadata": {
        "id": "RqLeNr98Ywst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Speichern mit Accuracy im Namen\n",
        "save_model(model, model_results[\"model_acc\"])"
      ],
      "metadata": {
        "id": "cMyvNouXZR3c"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}